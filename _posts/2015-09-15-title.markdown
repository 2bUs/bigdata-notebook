---
layout: post
title: Spark MapReduce and Scala underscore
date: 2015-09-15 15:09:00
description: A blog on how to write Spark mapreduce and an introduction on Scala underscore
---
This is a basic guide on how to run map-reduce in Apache Spark using Scala. I will also try to explain the basics of Scala underscore, how it works and few examples of writing map-reduce programs with and without using underscore. 

The source code is available <a href="https://github.com/soniclavier/hadoop/blob/master/map_reduce_in_spark.scala" target="blank">here</a>

#### MapReduce
The first step is to create an RDD(Resilient Distributed Dataset) of input Strings. An RDD is a collection of data which is partitioned, it is similar to a distributed collection. The more the number of partitions in an RDD, the more the parallelism. When a job runs, each partition will be moved to the node where it is going to be processed.

{% highlight scala %}
val lines = sc.parallelize(List("this is","an example"))
lines: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:21
{% endhighlight %}

<b>sc</b>, is the spark context which is available by default in the spark-shell and <b>sc.parallelilize</b> will parallelize the input which is a list of two Strings in this case. 

#### Map
Now that we have the RDD, we will run a <b>map()</b> operation on it.

{% highlight scala %}
val lengthOfLines = lines.map(line => (line.length))
lengthOfLines: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at map at <console>:23
lengthOfLines.collect()
res0: Array[Int] = Array(7, 10)
{% endhighlight %}

Here the map operation executes given function (to find the length) on each the element in the RDD and returns a new RDD. <i>lengthOfLines.collect()</i> operation shows that the result is an array with elements 7 and 10.

#### Reduce
Let us run the reduce operation on the result we obtained from map.
{% highlight scala %}
lines.map(line => (line.length)).reduce((a,b) => a + b)
res1: Int = 17
{% endhighlight %}

<b>reduce()</b> operation in Spark is a bit different from how the Hadoop MapReduce used to be, reduce() in spark produces only single output instead of producing key-value pairs. In the above reduce operation the length of each line, is summed up to obtain the result, 17. We will later go through <b>reduceByKey()</b> operation which is similar to the reduce in Hadoop MapReduce.

Let's take another example of reduce() 

{% highlight scala %}
val lines = sc.parallelize(List("this is","an example"))
val firstWords = lines.map(line => (line.substring(0,line.indexOf(" ")))).collect()
firstWords: Array[String] = Array(this, an)
firstWords.reduce((a,b) => a +" "+b)
res2: String = this an
{% endhighlight %}

In this example, we took two lines, did a substring operation to obtain only the first word of each line and then concatenated in reduce. The point to note here is that, reduce() operation always returns a single result - string "this an" in this example. 
<blockquote>Here we ran reduce operation on an Array(firstWords) which is not on an RDD. reduceByKey() can only called on an RDD where as reduce() can be called even if the object is not an RDD.</blockquote>